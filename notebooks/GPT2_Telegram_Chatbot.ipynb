{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2-Telegram-Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rer_47I95Kns",
        "colab_type": "text"
      },
      "source": [
        "# GPT2-Telegram-Chatbot\n",
        "A GPT-2 Telegram chatbot that's been relatively tuned for chatting. Feel free to make me PRs and I'll check out your code! The bot isn't 100% accurate all the time (why I coded in a /retry function.)\n",
        "\n",
        "Since the bot consumes so much memory, I have it programmed in a round-robin sort of mode. Each input will reset a timer on your account ID, once the timer runs down the bot is free for other users to use. You will be notified when the timer runs down, and other users can see how much time is left and if the bot is in use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pF534aJ5IP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed106c98-3852-4c7d-d4ba-59a0991c45b4"
      },
      "source": [
        "!pip3 install tensorflow-gpu===1.15.0\n",
        "!pip3 install tqdm\n",
        "!pip3 install regex\n",
        "!pip3 install fire\n",
        "!pip3 install python-telegram-bot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu===1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (0.35.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (1.18.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (1.12.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (1.32.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (1.1.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 58.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu===1.15.0) (0.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu===1.15.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu===1.15.0) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu===1.15.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu===1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu===1.15.0) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu===1.15.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=9b26eee8f7c4aacedf8d5f46ac5ad544eebff549f4664e18548fa9eee1664c9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, keras-applications, tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n",
            "Collecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=f2fa4df097176a6374eb809c04c6d65dfbb9d3376af18b8ab9fe463f58dcecc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.3.1\n",
            "Collecting python-telegram-bot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2d/c72fc9a28144277f6170f2fcbfd3bd9427943497522b2689846596eb86cf/python_telegram_bot-12.8-py2.py3-none-any.whl (375kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 8.6MB/s \n",
            "\u001b[?25hCollecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/9c/647e559a6e8be493dc2a7a5d15d26cb501ca60ec299b356f23839a673a83/cryptography-3.1-cp35-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 19.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (4.4.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (2020.6.20)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (5.1.1)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-telegram-bot) (1.15.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-telegram-bot) (1.14.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->python-telegram-bot) (2.20)\n",
            "Installing collected packages: cryptography, python-telegram-bot\n",
            "Successfully installed cryptography-3.1 python-telegram-bot-12.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrJpk10z5VdP",
        "colab_type": "text"
      },
      "source": [
        "## Clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56Tg6ouJ5yEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8a1f5e07-a7f4-49ab-e1d6-8e474b84cfbf"
      },
      "source": [
        "!git clone --depth=1 https://github.com/paper2code/GPT2-Telegram-Chatbot /content/gpt2telegram\n",
        "!ls -l /content/\n",
        "%cd /content/gpt2telegram\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/gpt2telegram'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 13 (delta 0), reused 6 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (13/13), done.\n",
            "total 8\n",
            "drwxr-xr-x 4 root root 4096 Sep 22 13:22 gpt2telegram\n",
            "drwxr-xr-x 1 root root 4096 Sep 16 16:29 sample_data\n",
            "/content/gpt2telegram\n",
            "total 28\n",
            "-rw-r--r-- 1 root root 1063 Sep 22 13:22 download_model.py\n",
            "-rw-r--r-- 1 root root 5899 Sep 22 13:22 README.md\n",
            "-rw-r--r-- 1 root root   36 Sep 22 13:22 requirements.txt\n",
            "-rw-r--r-- 1 root root 1058 Sep 22 13:22 SCORES.md\n",
            "drwxr-xr-x 2 root root 4096 Sep 22 13:22 src\n",
            "-rwxr-xr-x 1 root root   50 Sep 22 13:22 start\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Jbex7t6I40",
        "colab_type": "text"
      },
      "source": [
        "## Download model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FrBB_px6D_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ec8710e5-fcbe-441d-a903-88259d6fda28"
      },
      "source": [
        "!python3 download_model.py 1558M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 841kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 63.4Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 879kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 6.23Git [02:09, 48.0Mit/s]                                 \n",
            "Fetching model.ckpt.index: 21.0kit [00:00, 13.1Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 1.84Mit [00:00, 72.2Mit/s]                                                \n",
            "Fetching vocab.bpe: 457kit [00:00, 52.9Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsZKFiVs6Sr3",
        "colab_type": "text"
      },
      "source": [
        "## Setup the Telegram Bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrUQtJJv6WJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9cb1c65-bfa7-49aa-a044-7a608b45280e"
      },
      "source": [
        "!sed -i -e 's/BOTKEYBOTKEYBOTKEYBOTKEYBOTKEY/YOUR_TOKEN_HERE/' src/GPT2-Learning.py\n",
        "!cat src/GPT2-Learning.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#!/usr/bin/env python3\n",
            "# -*- coding: utf-8 -*-\n",
            "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n",
            "import fire, json, os, string, sys, threading, random, model, sample, encoder, logging, time\n",
            "import numpy as np\n",
            "import tensorflow as tf\n",
            "import re\n",
            "# Enable logging\n",
            "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
            "                    level=logging.INFO)\n",
            "logger = logging.getLogger(__name__)\n",
            "\n",
            "# Console output debug prints\n",
            "debug = True\n",
            "# Session timeout\n",
            "timstart = 1500\n",
            "# Model thinking per word 0.66 or 0.77 work well. \n",
            "top = 0.66\n",
            "# Temperature (refer to gpt-2 documentation)\n",
            "temp = 1\n",
            "# top_p multiplier - add to top_p per word \n",
            "# 0.00375‬ - may be shorter\n",
            "# 0.00400\n",
            "# 0.00425\n",
            "# 0.00450\n",
            "# 0.00475\n",
            "# 0.00500 - may be longer\n",
            "mx = 0.00375\n",
            "# This is the start of the learning context.\n",
            "learning = \"\"\n",
            "\n",
            "# End settings\n",
            "mode = False\n",
            "learn = False\n",
            "user = \"\"\n",
            "cache = \"\"\n",
            "running = False\n",
            "temps = str(temp)\n",
            "tpstring = str(top)\n",
            "\n",
            "# Define a few command handlers. These usually take the two arguments bot and\n",
            "# update. Error handlers also receive the raised TelegramError object in error.\n",
            "\n",
            "def start(bot, update):\n",
            "    \"\"\"Send a message when the command /start is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def help(bot, update):\n",
            "    \"\"\"Send a message when the command /help is issued.\"\"\"\n",
            "    update.message.reply_text('Just type a message... It could be lagged out. /chatbot goes into Me: You: mode. /finish just finishes the text /learnon for conversation learning mode.')\n",
            "\n",
            "def chatbot(bot, update):\n",
            "    \"\"\"Send a message when the command /chatbot is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = True\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = True\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def finish(bot, update):\n",
            "    \"\"\"Send a message when the command /finish is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = False\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = False\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def learnon(bot, update):\n",
            "    \"\"\"Send a message when the command /learnon is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def learnoff(bot, update):\n",
            "    \"\"\"Send a message when the command /learnoff is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = True\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = True\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def learnreset(bot, update):\n",
            "    \"\"\"Send a message when the command /learnreset is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 1558M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def regex(mew):\n",
            "    meow = mew\n",
            "    if \"You:\" in meow:\n",
            "        meow = meow[0:meow.find('You:')]\n",
            "        if \"Me:\" in meow:\n",
            "            meow = meow[0:meow.find('Me:')]\n",
            "        return meow\n",
            "    if \"Me:\" in meow:\n",
            "        meow = meow[0:meow.find('Me:')]\n",
            "        if \"You:\" in meow:\n",
            "            meow = meow[0:meow.find('You:')]\n",
            "        return meow\n",
            "    if \"?\" in meow:\n",
            "        meow = meow[0:meow.find('?')]\n",
            "        meow = meow + \"?\"\n",
            "        return meow\n",
            "    if \"!\" in meow:\n",
            "        meow = meow.rsplit('!', 1)[0]\n",
            "        meow = meow + \"!\"\n",
            "        return meow\n",
            "    else:\n",
            "        meow = meow.rsplit('.', 1)[0]\n",
            "        meow = meow + \".\"\n",
            "        return meow\n",
            "    meow = \"Error.\"\n",
            "    return meow\n",
            "\n",
            "\n",
            "def retry(bot, update):\n",
            "    retr = True\n",
            "    top_p = top\n",
            "    temperature = temp\n",
            "    mult = mx\n",
            "    new = retr\n",
            "    comput = threading.Thread(target=wait, args=(bot, update, top_p, temperature, mult, new,))\n",
            "    comput.start()\n",
            "\n",
            "def runn(bot, update):\n",
            "    retr = False\n",
            "    top_p = top\n",
            "    temperature = temp\n",
            "    mult = mx\n",
            "    new = retr\n",
            "    comput = threading.Thread(target=wait, args=(bot, update, top_p, temperature, mult, new,))\n",
            "    comput.start()\n",
            "\n",
            "def wait(bot, update, top_p, temperature, mult, new):\n",
            "    global tim\n",
            "    global user\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "    if user == update.message.from_user.id:\n",
            "        user = update.message.from_user.id\n",
            "        tim = timstart\n",
            "        compute = threading.Thread(target=interact_model, args=(bot, update, top_p, temperature, mult, new,))\n",
            "        compute.start()\n",
            "        if running == False:\n",
            "            while tim > 1:\n",
            "                running = True\n",
            "                time.sleep(1)\n",
            "                tim = tim - 1\n",
            "            if running == True:\n",
            "                mode = False\n",
            "                learn = False\n",
            "                learning = \"\"\n",
            "                cache = \"\"\n",
            "                user = \"\"\n",
            "                update.message.reply_text('Timer has run down, bot has been reset into the default mode.')\n",
            "                running = False\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is in use, current cooldown is: ' + left + ' seconds.')\n",
            "\n",
            "def interact_model(bot, update, top_p, temperature, mult, new):\n",
            "    model_name = '1558M'\n",
            "    seed = random.randint(1431655765, 2863311530)\n",
            "    # random.randint(1, 4294967295)\n",
            "    # random.randint(1073741824, 3221225471)\n",
            "    # random.randint(1431655765, 2863311530)\n",
            "    nsamples = 1\n",
            "    batch_size = 1\n",
            "    top_k = 0\n",
            "    models_dir = 'models'\n",
            "    tex = update.message.text\n",
            "    penguin = str(tex)\n",
            "    global learning\n",
            "    global learn\n",
            "    global mode\n",
            "    global cache\n",
            "#############################################\n",
            "    # This does some basic length processing.\n",
            "    if mode == True:\n",
            "        cat = len(penguin.split())\n",
            "        if cat > 300:\n",
            "            update.message.reply_text('Input text is too long.')\n",
            "            return\n",
            "        if new == True and cache:\n",
            "            m = re.search('.* You: ', cache)\n",
            "            raw_text = m.group(0)\n",
            "            cac = len(raw_text.split())\n",
            "            cat = cac - 2   \n",
            "            length = cat\n",
            "            if cat < 20:\n",
            "                length = 20\n",
            "            if cat > 20:\n",
            "                length = 20\n",
            "            if cat > 30:\n",
            "               length =  40\n",
            "            if cat > 50:\n",
            "                length = 60\n",
            "            if debug == True:\n",
            "                print(\"Cache is...\")\n",
            "                print(raw_text)\n",
            "        if new != True:\n",
            "            wolf = 'Me: ' + penguin\n",
            "            initial = wolf + ' You: '\n",
            "            raw_text = learning + initial\n",
            "            length = cat\n",
            "            if cat < 20:\n",
            "                length = 20\n",
            "            if cat > 20:\n",
            "                length = 20\n",
            "            if cat > 30:\n",
            "               length =  40\n",
            "            if cat > 50:\n",
            "                length = 60\n",
            "            cache = raw_text\n",
            "        tgt = len(raw_text.split())\n",
            "        if tgt > 300:\n",
            "            while tgt > 300:\n",
            "                if debug == True:\n",
            "                    print(\"Reducing memory of chat.\")\n",
            "                raw_text = raw_text.split(' Me:', 1)[-1]\n",
            "                raw_text = \"Me:\" + raw_text\n",
            "                tgt = len(raw_text.split())\n",
            "                if tgt > 300:\n",
            "                    if debug == True:\n",
            "                        print(\"Reducing memory of chat.\")\n",
            "                    raw_text = raw_text.split('You:', 1)[-1]\n",
            "                    raw_text = \"You:\" + raw_text\n",
            "                    tgt = len(raw_text.split())\n",
            "            if debug == True:\n",
            "                print(\"FINAL MEMORY REDUCTION:\")\n",
            "                print(raw_text)\n",
            "    if mode == False:\n",
            "        cat = len(penguin.split())\n",
            "        length = cat\n",
            "        if length > 300:\n",
            "            update.message.reply_text('Input text is too long.')\n",
            "            return\n",
            "        if new != True:\n",
            "            cache = penguin\n",
            "        if new == True and cache:\n",
            "            penguin = cache\n",
            "            length = len(penguin.split())\n",
            "            cat = length\n",
            "            if debug == True:\n",
            "                print(\"Cache is...\")\n",
            "                print(penguin)\n",
            "        raw_text = penguin\n",
            "    tx = float(top_p)\n",
            "    cax = float(cat)\n",
            "    cay = float(mx)\n",
            "    caz = float(cax * cay)\n",
            "    #ta = ((1-tx)/caz)\n",
            "    #top_p = ((tx) + (ta))\n",
            "    top_p = caz + tx\n",
            "    if top_p > 0.84:\n",
            "        top_p = 0.84\n",
            "    if top_p < 0.005:\n",
            "        top_p = 0.005\n",
            "#############################################\n",
            "    update.message.reply_text('Computing...')\n",
            "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
            "    if batch_size is None:\n",
            "        batch_size = 1\n",
            "    assert nsamples % batch_size == 0\n",
            "    enc = encoder.get_encoder(model_name, models_dir)\n",
            "    hparams = model.default_hparams()\n",
            "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
            "        hparams.override_from_dict(json.load(f))\n",
            "    if length is None:\n",
            "        length = hparams.n_ctx // 2\n",
            "    elif length > hparams.n_ctx:\n",
            "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
            "    with tf.Session(graph=tf.Graph()) as sess:\n",
            "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
            "        np.random.seed(seed)\n",
            "        tf.set_random_seed(seed)\n",
            "        output = sample.sample_sequence(\n",
            "            hparams=hparams, length=length,\n",
            "            context=context,\n",
            "            batch_size=batch_size,\n",
            "            temperature=temperature, top_k=top_k, top_p=top_p\n",
            "        )\n",
            "        saver = tf.train.Saver()\n",
            "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
            "        saver.restore(sess, ckpt)\n",
            "        context_tokens = enc.encode(raw_text)\n",
            "        generated = 0\n",
            "        for _ in range(nsamples // batch_size):\n",
            "            out = sess.run(output, feed_dict={\n",
            "                context: [context_tokens for _ in range(batch_size)]\n",
            "            })[:, len(context_tokens):]\n",
            "            for i in range(batch_size):\n",
            "                generated += 1\n",
            "                text = enc.decode(out[i])\n",
            "                if debug == True:\n",
            "                    print(\"==========\")\n",
            "                    print(\"Before splitlines: \" + text)\n",
            "                    print(\"==========\")\n",
            "                if mode == True:\n",
            "                    pika = text.splitlines()[0]\n",
            "                else:\n",
            "                    pika = text\n",
            "                stripes = pika.encode(encoding=sys.stdout.encoding,errors='ignore')\n",
            "                tigger = stripes.decode(\"utf-8\")\n",
            "                mew = str(tigger)\n",
            "                # disable any regex on finishsentence mode.\n",
            "                if mode == True:\n",
            "                    meo = regex(mew)\n",
            "                    meow = \" \".join(re.split(\"[^a-zA-Z.,?!'*]*\", meo))\n",
            "                    # Final regex\n",
            "                else:\n",
            "                    meow = mew\n",
            "                if learn == True:\n",
            "                    learning = raw_text + meow + \" \"\n",
            "                update.message.reply_text(meow)\n",
            "                if debug == True:\n",
            "                    print(\"==========\")\n",
            "                    mod = str(mode)\n",
            "                    print(\"Mode: \" + mod)\n",
            "                    lear = str(learn)\n",
            "                    print(\"Learn: \" + lear)\n",
            "                    lent = str(length)\n",
            "                    print(\"Length: \" + lent)\n",
            "                    print(\"==========\")\n",
            "                    ball = str(pika)\n",
            "                    print(\"Before regex: \" + ball)\n",
            "                    print(\"==========\")\n",
            "                    print(\"Output: \" + meow)\n",
            "                    print(\"==========\")\n",
            "                    print(\"Raw_text or Original: \" + raw_text)\n",
            "                    print(\"==========\")\n",
            "                    print(\"Learning text or Next: \" + learning)\n",
            "                    print(\"==========\")\n",
            "                    tps = str(top_p)\n",
            "                    print(\"top_p out: \" + tps)\n",
            "                    print(\"==========\")\n",
            "                    tpa = str(tx)\n",
            "                    print(\"top_p in: \" + tpa)\n",
            "                    print(\"==========\")\n",
            "    sess.close()\n",
            "\n",
            "def error(bot, update):\n",
            "    \"\"\"Log Errors caused by Updates.\"\"\"\n",
            "    logger.warning('Update \"%s\" caused error \"%s\"', update)\n",
            "\n",
            "def main():\n",
            "    \"\"\"Start the bot.\"\"\"\n",
            "    # Create the Updater and pass it your bot's token.\n",
            "    # Make sure to set use_context=True to use the new context based callbacks\n",
            "    # Post version 12 this will no longer be necessary\n",
            "    updater = Updater(\"1368252640:AAGHxUTVDuu_Lqy4BYh6F23kdKnkgFrK0-k\", use_context=False)\n",
            "    # Get the dispatcher to register handlers\n",
            "    dp = updater.dispatcher\n",
            "    # on different commands - answer in Telegram\n",
            "    dp.add_handler(CommandHandler(\"start\", start))\n",
            "    dp.add_handler(CommandHandler(\"help\", help))\n",
            "    dp.add_handler(CommandHandler(\"chatbot\", chatbot))\n",
            "    dp.add_handler(CommandHandler(\"finish\", finish))\n",
            "    dp.add_handler(CommandHandler(\"learnon\", learnon))\n",
            "    dp.add_handler(CommandHandler(\"learnoff\", learnoff))\n",
            "    dp.add_handler(CommandHandler(\"learnreset\", learnreset))\n",
            "    dp.add_handler(CommandHandler(\"retry\", retry))\n",
            "    # on noncommand i.e message - echo the message on Telegram\n",
            "    dp.add_handler(MessageHandler(Filters.text, runn))\n",
            "    # log all errors\n",
            "    dp.add_error_handler(error)\n",
            "    # Start the Bot\n",
            "    updater.start_polling()\n",
            "    # Run the bot until you press Ctrl-C or the process receives SIGINT,\n",
            "    # SIGTERM or SIGABRT. This should be used most of the time, since\n",
            "    # start_polling() is non-blocking and will stop the bot gracefully.\n",
            "    updater.idle()\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    fire.Fire(main)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dcAf0OJ7BXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9610082b-1627-499d-f04a-3f3aea89905e"
      },
      "source": [
        "!./start"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src/GPT2-Learning.py:544: TelegramDeprecationWarning: Old Handler API is deprecated - see https://git.io/fxJuV for details\n",
            "  updater = Updater(\"1368252640:AAGHxUTVDuu_Lqy4BYh6F23kdKnkgFrK0-k\", use_context=False)\n",
            "what is arxiv ?\n",
            "WARNING:tensorflow:From src/GPT2-Learning.py:466: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-09-22 13:26:42,374 - tensorflow - WARNING - From src/GPT2-Learning.py:466: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-09-22 13:26:42.379367: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-09-22 13:26:42.425549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:26:42.426147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-09-22 13:26:42.443489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-09-22 13:26:42.651184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-09-22 13:26:42.740001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-09-22 13:26:42.765870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-09-22 13:26:43.011239: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-09-22 13:26:43.139927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-09-22 13:26:43.654994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-09-22 13:26:43.655275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:26:43.656010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:26:43.656571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-09-22 13:26:43.658664: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2020-09-22 13:26:43.677273: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000165000 Hz\n",
            "2020-09-22 13:26:43.677553: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f888c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-09-22 13:26:43.677582: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-09-22 13:26:43.826139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:26:43.826839: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f89880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-09-22 13:26:43.826867: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-09-22 13:26:43.828137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:26:43.828786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-09-22 13:26:43.828860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-09-22 13:26:43.828884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-09-22 13:26:43.828906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-09-22 13:26:43.828933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-09-22 13:26:43.828951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-09-22 13:26:43.828969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-09-22 13:26:43.828989: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-09-22 13:26:43.829061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:26:43.829606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:26:43.830137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-09-22 13:26:43.833455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-09-22 13:26:43.834762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-09-22 13:26:43.834793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-09-22 13:26:43.834804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-09-22 13:26:43.844304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:26:43.844934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:26:43.845464: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-09-22 13:26:43.845506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From src/GPT2-Learning.py:467: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "2020-09-22 13:26:43,846 - tensorflow - WARNING - From src/GPT2-Learning.py:467: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From src/GPT2-Learning.py:469: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "2020-09-22 13:26:43,855 - tensorflow - WARNING - From src/GPT2-Learning.py:469: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt2telegram/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "2020-09-22 13:26:43,856 - tensorflow - WARNING - From /content/gpt2telegram/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt2telegram/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "2020-09-22 13:26:43,856 - tensorflow - WARNING - From /content/gpt2telegram/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt2telegram/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "2020-09-22 13:26:43,863 - tensorflow - WARNING - From /content/gpt2telegram/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt2telegram/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "2020-09-22 13:26:43,900 - tensorflow - WARNING - From /content/gpt2telegram/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt2telegram/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2020-09-22 13:26:50,728 - tensorflow - WARNING - From /content/gpt2telegram/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt2telegram/src/sample.py:39: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2020-09-22 13:26:50,745 - tensorflow - WARNING - From /content/gpt2telegram/src/sample.py:39: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt2telegram/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "2020-09-22 13:26:50,745 - tensorflow - WARNING - From /content/gpt2telegram/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/GPT2-Learning.py:476: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2020-09-22 13:26:56,911 - tensorflow - WARNING - From src/GPT2-Learning.py:476: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from models/1558M/model.ckpt\n",
            "2020-09-22 13:26:57,288 - tensorflow - INFO - Restoring parameters from models/1558M/model.ckpt\n",
            "2020-09-22 13:27:36.932858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:27:36.933921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-09-22 13:27:36.934129: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-09-22 13:27:36.934183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-09-22 13:27:36.934204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-09-22 13:27:36.934224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-09-22 13:27:36.934244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-09-22 13:27:36.934264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-09-22 13:27:36.934284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-09-22 13:27:36.934398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:27:36.935368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:27:36.936155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-09-22 13:27:36.936212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-09-22 13:27:36.936229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-09-22 13:27:36.936242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-09-22 13:27:36.936346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:27:36.936908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:27:36.937432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from models/1558M/model.ckpt\n",
            "2020-09-22 13:27:51,682 - tensorflow - INFO - Restoring parameters from models/1558M/model.ckpt\n",
            "2020-09-22 13:28:13.740388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:28:13.740998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-09-22 13:28:14.543049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-09-22 13:28:14.543182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-09-22 13:28:14.543210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-09-22 13:28:14.543233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-09-22 13:28:14.543254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-09-22 13:28:14.543274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-09-22 13:28:14.543295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-09-22 13:28:14.543413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:28:14.544061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:28:14.544558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-09-22 13:28:14.544622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-09-22 13:28:14.544638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-09-22 13:28:14.544658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-09-22 13:28:14.544771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:28:14.545357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-22 13:28:14.545932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from models/1558M/model.ckpt\n",
            "2020-09-22 13:28:29,517 - tensorflow - INFO - Restoring parameters from models/1558M/model.ckpt\n",
            "2020-09-22 13:29:03.129935: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 8589934592 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
            "2020-09-22 13:29:03.137739: W ./tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 8589934592\n",
            "2020-09-22 13:29:03.246474: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 7730940928 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
            "2020-09-22 13:29:03.246521: W ./tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 7730940928\n",
            "2020-09-22 13:29:03.246562: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 6957846528 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
            "2020-09-22 13:29:03.246576: W ./tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 6957846528\n",
            "2020-09-22 13:29:03.246608: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 6262061568 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
            "2020-09-22 13:29:03.246622: W ./tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 6262061568\n",
            "2020-09-22 13:29:03.246653: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 5635855360 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
            "2020-09-22 13:29:03.246666: W ./tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 5635855360\n",
            "2020-09-22 13:29:03.246712: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 5072269824 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
            "2020-09-22 13:29:03.246725: W ./tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 5072269824\n",
            "2020-09-22 13:29:03.246758: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 4565042688 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
            "2020-09-22 13:29:03.246771: W ./tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 4565042688\n",
            "./start: line 4:   236 Killed                  python3 src/GPT2-Learning.py\n",
            "src/GPT2-Learning.py:544: TelegramDeprecationWarning: Old Handler API is deprecated - see https://git.io/fxJuV for details\n",
            "  updater = Updater(\"1368252640:AAGHxUTVDuu_Lqy4BYh6F23kdKnkgFrK0-k\", use_context=False)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}